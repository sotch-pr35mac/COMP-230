Kalamazoo College CS230 Computer Organization: Notes Part 4

- Register File: Read 2 Registers Simultaneously
	- Decoder is the opposite of a Mux, it handles input to the registers
	- The Mux handles output from the registers

- Memory
	- how do you keep the data from changing?
		- a set bit and a get bit
		- you need to use feedback loops

- Single Cycle CPU
	- Control Line Names
		A RegDst
		B Jump
		C Branch
		D MemWrite
		E MemToReg
		F ALUOp
		G MemRead
	- Each step takes a certain amount of time

- Measuring Time
	- Two versions
		- Repsonse time - how fast does a particular task get done?
		- Throughput - the amount of x that can go through the CPU
- Measuring Response Time
	- Elapsed time vs CPU time
		- Elapsed time = Wall-Clock Time
		- CPU Time = amount of time the computer is working on our task
	- We will lookat CPU Time
		- Why?

- Measuring CPU Time
	- User CPU time vs. System CPU time
		- User CPU time: amount of time spent executing the user's task
		- System CPU time: amount of time spent executing tasks related to the user's task
			- This is hard to measure, and depends on operating system etc.
	- We'll focus on User CPU time

- Finding User CPU Time
	- Experimentation:
		- UNIX time utility
	- Calculations:
		- Several formulas for calculating CPU Performance
	- Note: Performance is inserve of execution time -- as execution time decreases, performance increases / improves

- Clock Rate
	- All computers have a clock running at a constant rate
	- Clock period is the length of a single clock tick (cycle)
	- Clock rate is inverse of clock period
		- Clock period = .5 nanoseconds
		- Clock rate = 2GHz

- Clock Rate and Performance
	- measuring Performance using the clock
		- execution time = number of cycles for the program * time for one clock cycle

- How many cycles does the program take
	- How do we know how many cycles a program is going to need?
		number of cycles for program = number of instructions * cycles per instruction
	- number of instructinos = instruction count (IC)
	- Cycles for one instruction = cycles per instruction (CPI)
	- So, clock cycles = IC * CPI
	- However, lets think about what happens when instructions take a different amount of cycles to execute

- New Formula
	- Put his in our formula
		- Execution time = ic * cpi * clock period
		OR
		- Execution time = (IC * CPI) / Clock Rate
	- This is the basic performance equation

- How do the units work out?
	- IC = instructions in program
	- CPI = cycles per instruction
	- Clock period = seconds per cycle
	- Execution Time = Seconds/program

- Finding CPI
	- Use a weighted average
	- Example
		- 25% floating point operations 4 cycles
		- 75% integer operations 1 cycle
		- What is the CPI?
			- 4 * 25% + 1 * 75% = 1.75

- Amdahl's Law
	- The performance enchancement possible with a given improvement is limited by the amount that the impoved feature is used
	- Floating point operations twice as fast, but, imporved machine is only 1.4 times as fast.
	- Moral, MAKE THE COMMON CASE

- So now what?
	- Time to compare two machines
	- Need to choose a program or set of programs to test both machines with
	- Which programs to choose?
	- Set of programs is called a benchmark
		- The book talks about the SPEC benchmark

- Choosing Benchmarking programs
	- standarized
		- allow comparison among many machines
	- small vs large	
		- small
			- Easier to run many times
			- easier to "cheat"
		- large	
			- more realistic
			- harder to standarize

- Energy Efficiency
	- Performance / Power Consumption

- Performance issues
	- Longest delay determines the clock speed
	- RISK has simple instructions, to be able to make the common case fast
	- So what we have is pipline.

- Pipelined Laundy: overlapping execution
	- Parallelism improves performance
		- We don't have parallelism for the purpose of this part of notes
	- So each laundy load takes the same amount of time, and we have four loads of laundry to do, we have to wait for one load to be completely done before we start the next, we put one in the dryer and then put one in the washer. 
		- This idea here will speed up the throughput, and we are not speeding up the clock, we are just making it so that more instructions get through the CPU in a less amount of time, speeding up the throughput. 

- MIPS Pipeline
	- Gives Ttages, one step per stage

- Software: no-op
- Hardware: stall

- Branch Prediction
	- Longer pipelines can't readily determine branch outcome early
		- Stall penalty becomes unacceptable
	- Predict outcome of branch
		- ONly stall if prediction is wrong
	- In MIPS Pipeline
		- Can predict branches not taken
		- Getch instruction only after the delay

- Main Memory (DRAM) much lsower than processors.
- Gap has widened over the past 20+ years, continues to widen
- Memory comes in different technologies: fastest is also the most expensive

- Temporal locality - likely to reuse data soon - WHY?
	- likely to reuse data soon (loops)
- Spatial locality - likely to access data close by - WHY?
	- likely to access data close by (seqential nature of program instructions)

- Let's Talk About Caching
	- Check if data is in fast memory (hit); if not (miss), fetch a block from slower memory - "Block" may be a single word or several words
	- Fetch tiem is miss penalty
	- Hit rate is a fraction of memory accesses that are hits; miss rate = 1 - hit rate
	- If locality is high, hti rate gets closer to 1
	- Effect: memory is almost as fast as fastest level, as big as bigggest (slowest) memory
	- Direct Mapped:
		- Any given memory address, can only be found in once cache address (although many memory addresses will mpa to the same cache address)
	- 256 Byte Cache (64 4-byte words)
	- Each Cache "line" or "block" holds one word (4 bytes)
	- Byte in chche is addressed by lowest two bits of address
	- Cache line is addressed by ext six bts in address
	- Each Cache line has a "Tag" matching the high 24 bits of memory address
	- Write Through
		- Write to memory and to cache
		- Time to write to memory could delay instruction --> write buffer can hide this latency
		- Write Back (also called Copy Back)
			- Write only to cache
			--> mark cache line as "dirty", using and additional bit
			- When cache line is replaced, if dirty, then write back to memory

- Accelerating Memory Access
	- Wider bus
	- Wider memory access
	- How can Cache take advangtage of faster memory access?
	- Store more than one word at a time on each "line" in the cache
		- Any cahce miss brings the whole line containing the item into the cache
	- Takes advantage of spatial locality
		- next tiem needed is likely to be at the next address

- Cache with mulit-word lines
	- 256 Byte cache - 64 4 byte words
	- Each block (line) contains four words (16 bytes)
		- 2 bits to address byte in word
		- 2 bits to address word in line
	-Cache contains sizteen four-word blocks
		- 4 bits to address cache block (line)
	- Each cache lin ehas tag field for the upper 24 bits of the address

- Speeding up Cache
	- execution time = (execution cycles + stall cycles) * cycle time
		stall cycles = # of instructions * miss ratio * miss penalty
	- We can imporve performance by
		- Reducing miss rate
		- Reducing miss penalty
	- More flexible palcement of blocks
		- Miss rate
	- Caching the Cache
		- Miss penalty
